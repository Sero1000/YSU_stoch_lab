{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB: Stochastic Gradient Descent\n",
    "\n",
    "The objective of this lab session is to implement:\n",
    "- Stochastic gradient descent with constant stepsizes\n",
    "- Stochastic gradient descent with shrinking stepsizes\n",
    "- Stochastic gradient descent with sampling with/without replacement\n",
    "- Stochastic gradient descent with averaging \n",
    "- SAG, Stochastic average gradient descent\n",
    "\n",
    "and compare your implementation with gradient descent.\n",
    "\n",
    "Throughout the notebook you will find commented boxes like this one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO ###   \n",
    "# please implement blabla\n",
    "#############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These boxes need to be replaced by code as explained in the boxes.\n",
    "Solutions will online tomorrow. Good luck!\n",
    "\n",
    "Robert Gower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import norm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='loss'></a>\n",
    "## 1. Loss functions, gradients and step-sizes\n",
    "\n",
    "We want to minimize\n",
    "$$\n",
    "\\frac 1n \\sum_{i=1}^n \\ell(x_i^\\top w, b_i) + \\frac \\lambda 2 \\|w\\|_2^2\n",
    "$$\n",
    "where\n",
    "- $\\ell(z, b) = \\frac 12 (b - z)^2$ (least-squares regression)\n",
    "- $\\ell(z, b) = \\log(1 + \\exp(-bz))$ (logistic regression).\n",
    "\n",
    "We write it as a a minimization problem of the form\n",
    "$$\n",
    "\\frac 1n \\sum_{i=1}^n f_i(w)\n",
    "$$\n",
    "where\n",
    "$$\n",
    "f_i(w) = \\ell(x_i^\\top w, y_i) + \\frac \\lambda 2 \\|w\\|_2^2.\n",
    "$$\n",
    "\n",
    "For both cases, the gradients are\n",
    "$$\n",
    "\\nabla f_i(w) = (x_i^\\top w - y_i) x_i + \\lambda w\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\nabla f_i(w) = - \\frac{y_i}{1 + \\exp(y_i x_i^\\top w)} x_i + \\lambda w.\n",
    "$$\n",
    "\n",
    "Denote by $L$ the Lipschitz constant of $f$ and $X = [x_1, \\ldots, x_n].$\n",
    "One can see easily that for linear regression\n",
    "$$\n",
    "L = \\frac{ \\|\\mathbf X \\mathbf X^\\top \\|_{2}}{n} + \\lambda \n",
    "$$\n",
    "while for logistic regression it is\n",
    "$$\n",
    "L = \\frac{ \\|\\mathbf X \\mathbf X^\\top \\|_{2}}{4 n} + \\lambda \n",
    "$$\n",
    "For full-gradient methods, the theoretical step-size is $1 / L$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now introduce a class that will be used for the solvers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import svd\n",
    "\n",
    "class LinReg(object):\n",
    "    \"\"\"A class for the least-squares regression with\n",
    "    Ridge penalization\"\"\"\n",
    "\n",
    "    def __init__(self, X, y, lbda):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.n, self.d = X.shape\n",
    "        self.lbda = lbda\n",
    "    \n",
    "    def grad(self, w):\n",
    "        ### TODO ###   \n",
    "        # calculate the gradient of f\n",
    "        #############\n",
    "        return 0\n",
    "    \n",
    "    def f(self, w):\n",
    "        return norm(self.X.dot(w) - self.y) ** 2 / (2. * self.n) + self.lbda * norm(w) ** 2 / 2.\n",
    " \n",
    "    def f_i(self, i, w):\n",
    "        return norm(self.X[i].dot(w) - self.y[i]) ** 2 / (2.) + self.lbda * norm(w) ** 2 / 2.\n",
    "    \n",
    "    def grad_i(self, i, w):\n",
    "        ### TODO ###   \n",
    "        # calculate the gradient of f_i\n",
    "        #############\n",
    "        return 0\n",
    "\n",
    "    def lipschitz_constant(self):\n",
    "        \"\"\"Return the Lipschitz constant of the gradient\"\"\"\n",
    "        L = norm(self.X, ord=2) ** 2 / self.n + self.lbda\n",
    "        return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogReg(object):\n",
    "    \"\"\"A class for the logistic regression with L2 penalization\"\"\"\n",
    "\n",
    "    def __init__(self, X, y, lbda):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.n, self.d = X.shape\n",
    "        self.lbda = lbda\n",
    "    \n",
    "    def grad(self, w):\n",
    "        ### TODO ###   \n",
    "        # calculate the gradient of f\n",
    "        #############\n",
    "        return 0\n",
    "\n",
    "    def f(self, w):\n",
    "        bAx = self.y * np.dot(self.X, w)\n",
    "        return np.mean(np.log(1. + np.exp(- bAx))) + self.lbda * norm(w) ** 2 / 2.\n",
    "    \n",
    "    def f_i(self,i, w):\n",
    "        bAx_i = self.y[i] * np.dot(self.X[i], w)\n",
    "        return np.log(1. + np.exp(- bAx_i)) + self.lbda * norm(w) ** 2 / 2.\n",
    "    \n",
    "    def grad_i(self, i, w):\n",
    "        ### TODO ###   \n",
    "        # calculate the gradient of f_i\n",
    "        #############\n",
    "        return grad\n",
    "\n",
    "    def lipschitz_constant(self):\n",
    "        \"\"\"Return the Lipschitz constant of the gradient\"\"\"\n",
    "        L = norm(self.X, ord=2) ** 2  / (4. * self.n) + self.lbda\n",
    "        return L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='data'></a>\n",
    "## 2. Generate a dataset\n",
    "\n",
    "We generate datasets for the least-squares and the logistic cases. First we define a function for the least-squares case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import multivariate_normal, randn\n",
    "from scipy.linalg.special_matrices import toeplitz\n",
    "\n",
    "    \n",
    "def simu_linreg(w, n, std=1., corr=0.5):\n",
    "    \"\"\"\n",
    "    Simulation of the least-squares problem\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : np.ndarray, shape=(d,)\n",
    "        The coefficients of the model\n",
    "    \n",
    "    n : int\n",
    "        Sample size\n",
    "    \n",
    "    std : float, default=1.\n",
    "        Standard-deviation of the noise\n",
    "\n",
    "    corr : float, default=0.5\n",
    "        Correlation of the features matrix\n",
    "    \"\"\"    \n",
    "    d = w.shape[0]\n",
    "    cov = toeplitz(corr ** np.arange(0, d))\n",
    "    X = multivariate_normal(np.zeros(d), cov, size=n)\n",
    "    noise = std * randn(n)\n",
    "    y = X.dot(w) + noise\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simu_logreg(w, n, std=1., corr=0.5):\n",
    "    \"\"\"\n",
    "    Simulation of the logistic regression problem\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : np.ndarray, shape=(d,)\n",
    "        The coefficients of the model\n",
    "    \n",
    "    n : int\n",
    "        Sample size\n",
    "    \n",
    "    std : float, default=1.\n",
    "        Standard-deviation of the noise\n",
    "\n",
    "    corr : float, default=0.5\n",
    "        Correlation of the features matrix\n",
    "    \"\"\"    \n",
    "    X, y = simu_linreg(w, n, std=1., corr=0.5)\n",
    "    return X, np.sign(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 50\n",
    "n = 1000\n",
    "idx = np.arange(d)\n",
    "\n",
    "# Ground truth coefficients of the model\n",
    "w_model_truth = (-1)**idx * np.exp(-idx / 10.)\n",
    "\n",
    "X, y = simu_linreg(w_model_truth, n, std=1., corr=0.1)\n",
    "#X, y = simu_logreg(w_model_truth, n, std=1., corr=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choice of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbda = 1. / n ** (0.5)\n",
    "model = LinReg(X, y, lbda)\n",
    "#model = LogReg(X, y, lbda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nG4Zh9Bgm7IZhGD3G/w8VcJ8AP5fBMAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f254fde1a58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(1, 2, 1)\n",
    "plt.stem(y); \n",
    "plt.subplot(1, 2, 2)\n",
    "plt.stem(w_model_truth); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.0164204595155546"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.optimize import check_grad\n",
    "# Check that the gradient and the loss numerically match\n",
    "check_grad(model.f, model.grad, np.random.randn(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-88-99502eb6975b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_i\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mgrad_error\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_i\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mind\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mvec\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_i\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mind\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0meps\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_i\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_error\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_error\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mstem\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   3467\u001b[0m                       mplDeprecation)\n\u001b[1;32m   3468\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3469\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3470\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3471\u001b[0m         \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwashold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1708\u001b[0m                     warnings.warn(msg % (label_namer, func.__name__),\n\u001b[1;32m   1709\u001b[0m                                   RuntimeWarning, stacklevel=2)\n\u001b[0;32m-> 1710\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1711\u001b[0m         \u001b[0mpre_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1712\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpre_doc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mstem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2498\u001b[0m         markerline, = self.plot(x, y, color=markercolor, linestyle=markerstyle,\n\u001b[0;32m-> 2499\u001b[0;31m                                 marker=markermarker, label=\"_nolegend_\")\n\u001b[0m\u001b[1;32m   2500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2501\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbottom\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 1)"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAD8CAYAAAB6paOMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff26219b588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check that the gradient grad_i and the numerical gradient of f_i agree\n",
    "grad_error = []\n",
    "for i in range(n):\n",
    "    ind = np.random.choice(n,1)\n",
    "    w =  np.random.randn(d)\n",
    "    vec =  np.random.randn(d)\n",
    "    eps = pow(10.0, -7.0)\n",
    "    model.f_i(ind[0],w)\n",
    "    grad_error.append((model.f_i( ind[0], w+eps*vec) - model.f_i( ind[0], w))/eps - np.dot(model.grad_i(ind[0],w),vec))\n",
    "plt.stem(grad_error); \n",
    "print(np.mean(grad_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find a highly accurate solution using LBFGS method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "\n",
    "w_init = np.zeros(d)\n",
    "w_min, obj_min, _ = fmin_l_bfgs_b(model.f, w_init, model.grad, args=(), pgtol=1e-30, factr =1e-30)\n",
    "\n",
    "print(obj_min)\n",
    "print(norm(model.grad(w_min)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='constant'></a> \n",
    "\n",
    "## 3. Implementing Stochastic Gradient Descent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD\n",
    "\n",
    "We recall that an iteration of SGD writes\n",
    "\n",
    "\n",
    "\n",
    "**for** $t = 1, \\ldots, T$ \n",
    "  \n",
    "$\\qquad$ Pick $i$ uniformly at random in $\\{1, \\ldots, n\\}$\n",
    "   \n",
    "$\\qquad \\displaystyle\n",
    "w^{t+1} \\gets w^t - \\eta_t \\nabla f_i(w_t)\n",
    "$\n",
    "  \n",
    "**end for**\n",
    "\n",
    "\n",
    "\n",
    "Complete the code below. The inputs are\n",
    "- n_iter: The number of iterations\n",
    "- indices: an np.array of indices of length n_iter. The indices[k]  is the index of stochastic gradient that will be used on the kth iteration. \n",
    "- steps: an np.array of positive floats of length n_iter. The steps[k] is the stepsize used on the kth iteration. Typically decreasing stepsizes are used $\\eta_t = \\frac{\\eta_0}{\\sqrt{t+1}}$, where $\\eta_0$ is a step-size to be tuned by hand.\n",
    "\n",
    "- averaging_on: is a boolean which indicates if the output should be the average of the iterates.\n",
    "\n",
    "The outputs are:\n",
    "- w_output: The final w vector found by the algorithm or the average  $ \\bar{w} = \\frac{1}{T-t}\\sum_{i=t}^T w^t$ if averaging is on\n",
    "- objectives: A ndarray containing the sequence function values calculated during the iterations of the algorithm \n",
    "- errors: If w_min is not empty, errors is a ndarray containing the sequence of errors || w - w_min || of the algorithm. Otherwise errors should be empty.\n",
    "*QUESTION*:\n",
    "- Fill in the iteration of the SGD solver in the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(w0, model, indices, steps, w_min, n_iter=100, averaging_on=False ,verbose=True, start_late_averaging = 0):\n",
    "    \"\"\"Stochastic gradient descent algorithm\n",
    "    \"\"\"\n",
    "    w = w0.copy()\n",
    "    w_new = w0.copy()\n",
    "    n_samples, n_features = X.shape\n",
    "    # average x\n",
    "    w_average = w0.copy()\n",
    "    # estimation error history\n",
    "    errors = []\n",
    "    err = 1.0\n",
    "    # objective history\n",
    "    objectives = []\n",
    "    # Current estimation error\n",
    "    if np.any(w_min):\n",
    "        err = norm(w - w_min) / norm(w_min)\n",
    "        errors.append(err)\n",
    "    # Current objective\n",
    "    obj = model.f(w) \n",
    "    objectives.append(obj)\n",
    "    if verbose:\n",
    "        print(\"Lauching SGD solver...\")\n",
    "        print(' | '.join([name.center(8) for name in [\"it\", \"obj\", \"err\"]]))\n",
    "    for k in range(n_iter):\n",
    "        ### TODO ###   \n",
    "        ####################################\n",
    "        # Compute the next iterate\n",
    "        #  w_new[:]  = ........\n",
    "        ####################################\n",
    "        w[:] = w_new\n",
    "        ####################################\n",
    "        # Compute the average iterate \n",
    "        # w_average[:]  = ...w_average + .....\n",
    "        ####################################\n",
    "        if averaging_on:\n",
    "            w_test = w_average.copy()\n",
    "        else:\n",
    "            w_test = w.copy()\n",
    "        obj = model.f(w_test) \n",
    "        if np.any(w_min):\n",
    "            err = norm(w_test - w_min) / norm(w_min)\n",
    "            errors.append(err)\n",
    "        objectives.append(obj)\n",
    "        if k % n_samples == 0 and verbose:\n",
    "            if(sum(w_min)):\n",
    "                print(' | '.join([(\"%d\" % k).rjust(8), \n",
    "                              (\"%.2e\" % obj).rjust(8), \n",
    "                              (\"%.2e\" % err).rjust(8)]))\n",
    "            else:\n",
    "                print(' | '.join([(\"%d\" % k).rjust(8), \n",
    "                              (\"%.2e\" % obj).rjust(8)]))\n",
    "    if averaging_on:\n",
    "        w_output = w_average.copy()\n",
    "    else:\n",
    "        w_output = w.copy()    \n",
    "    return w_output, np.array(objectives), np.array(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Setup number of iterations\n",
    "datapasses = 15  # number of sweeps through all the data. This means that there will datapasses*n stochastic gradient updates\n",
    "niters = int(datapasses * n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD with constant step with replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##############TODO######################\n",
    "# Execute SGD with a constant stepsize. Please name the output as\n",
    "# w_sgdcr, obj_sgdcr, err_sgdcr = sgd(...?....)\n",
    "# HINT: You will have to guess a stepsizes and see if it works! Something around 0.005 should work here.\n",
    "##############END TODO######################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD with $1/\\sqrt(t)$ stepsizes with replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############TODO#######################\n",
    "# Execute SGD with a shrinking stepsize \\eta_t = C/\\sqrt(t). Please name the output as\n",
    "# w_sgdsr, obj_sgdsr, err_sgdsr = sgd(.....?.....)\n",
    "# HINT: You will have to guess C and see if it works! Something around C = 0.1 should work here.\n",
    "##############END TODO######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error of objective on a logarithmic scale\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.semilogy(obj_sgdcr - obj_min, label=\"SGD const\", lw=2)\n",
    "plt.semilogy(obj_sgdsr - obj_min, label=\"SGD shrink\", lw=2)\n",
    "plt.title(\"Convergence plot\", fontsize=16)\n",
    "plt.xlabel(\"#iterations\", fontsize=14)\n",
    "plt.ylabel(\"Error of objective\", fontsize=14)\n",
    "plt.legend()\n",
    "# Distance to the minimizer on a logarithmic scale\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.yscale(\"log\")\n",
    "plt.semilogy(err_sgdcr , label=\"SGD const\", lw=2)\n",
    "plt.semilogy(err_sgdsr , label=\"SGD shrink\", lw=2)\n",
    "plt.title(\"Convergence plot\", fontsize=16)\n",
    "plt.xlabel(\"#iterations\", fontsize=14)\n",
    "plt.ylabel(\"Distance to the minimum\", fontsize=14)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**\n",
    "\n",
    "- Compare the solution you obtain for SGD with constant stepsizes and SGD with shrinking stepsizes. \n",
    "- Which one is faster in the beginning? Which reaches the \"best\" solution?\n",
    "- What happens when is you use sampling without replacement instead? Hint: Do only one datapass, it's annoying to adapt this implementation for more than one datapass when sampling without replacement\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compare with averaging step\n",
    "\n",
    "- Implement the average iterate output  \n",
    "- Compare the solution you obtain for SGD with shrinking stepsizes and SGD with averaging. \n",
    "- What happens if you start averaging only the last n iterates? When is averaging useful?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############TODO#######################\n",
    "# Execute SGD with averaging on and shrinking stepsize. Please name the output as\n",
    "# w_sgdar, obj_sgdar, err_sgdar = sgd( .... )\n",
    "# HINT: You can use the stepsize you found in the last step.\n",
    "###############END TODO#####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error of objective on a logarithmic scale\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.semilogy(obj_sgdsr - obj_min, label=\"SGD shrink\", lw=2)\n",
    "plt.semilogy(obj_sgdar - obj_min, label=\"SGD average\", lw=2)\n",
    "plt.title(\"Convergence plot\", fontsize=16)\n",
    "plt.xlabel(\"#iterations\", fontsize=14)\n",
    "plt.ylabel(\"Loss function\", fontsize=14)\n",
    "plt.legend()\n",
    "# Distance to the minimizer on a logarithmic scale\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.semilogy(err_sgdsr , label=\"SGD shrink\", lw=2)\n",
    "plt.semilogy(err_sgdar , label=\"SGD average\", lw=2)\n",
    "plt.title(\"Convergence plot\", fontsize=16)\n",
    "plt.xlabel(\"#iterations\", fontsize=14)\n",
    "plt.ylabel(\"Distance to the minimum\", fontsize=14)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare with gradient descent\n",
    "\n",
    "- Complete the code of gradient descent (GD) below\n",
    "- How much more is the computational cost of a step of gradient descent with respect to the computational cost of a SGD step?  How many steps of gradient descent should you take so that the total computational complexity is equivalent to datapasses * n steps of SGD ? \n",
    "- Compare GD with SGD, where on the $x$-axis of the plot you the total computational effort spent\n",
    "- What happens if you increase the number of datapasses?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd(w0, model, step, w_min =[], n_iter=100, verbose=True):\n",
    "    \"\"\"Gradient descent algorithm\n",
    "    \"\"\"\n",
    "    w = w0.copy()\n",
    "    w_new = w0.copy()\n",
    "    n_samples, n_features = X.shape\n",
    "    # estimation error history\n",
    "    errors = []\n",
    "    err = 1.\n",
    "    # objective history\n",
    "    objectives = []\n",
    "    # Current estimation error\n",
    "    if np.any(w_min):\n",
    "        err = norm(w - w_min) / norm(w_min)\n",
    "        errors.append(err)\n",
    "    # Current objective\n",
    "    obj = model.f(w) \n",
    "    objectives.append(obj)\n",
    "    if verbose:\n",
    "        print(\"Lauching GD solver...\")\n",
    "        print(' | '.join([name.center(8) for name in [\"it\", \"obj\", \"err\"]]))\n",
    "    for k in range(n_iter ):\n",
    "        ##### TODO ######################\n",
    "        ##### Compute gradient step update\n",
    "        #####   w_new[:] = ...\n",
    "        ##### END TODO ##################\n",
    "        w[:] = w_new\n",
    "        obj = model.f(w) \n",
    "        if (sum(w_min)):\n",
    "            err = norm(w - w_min) / norm(w_min)\n",
    "            errors.append(err)\n",
    "        objectives.append(obj)\n",
    "        if verbose:\n",
    "            print(' | '.join([(\"%d\" % k).rjust(8), \n",
    "                              (\"%.2e\" % obj).rjust(8), \n",
    "                              (\"%.2e\" % err).rjust(8)]))\n",
    "    return w, np.array(objectives), np.array(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of iterations of GD so that the total computational effort is the same as your execution of SGD.\n",
    "complexityofGD = n * np.arange(0, datapasses + 1)\n",
    "print(complexityofGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############TODO######################\n",
    "# Execute the gradient algorithm. Please name the output as\n",
    "# w_gd, obj_gd, err_gd = gd(.....)\n",
    "# Hint: Remember the stepsize of gradient descent should be equal to 1/L, where L is this Lipshitz constant of f\n",
    "###############END TODO#####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error of objective on a logarithmic scale\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.semilogy(complexityofGD, obj_gd - obj_min, label=\"gd\", lw=2)\n",
    "plt.semilogy(obj_sgdsr - obj_min, label=\"sgd\", lw=2)\n",
    "plt.title(\"Convergence plot\", fontsize=16)\n",
    "plt.xlabel(\"# SGD iterations\", fontsize=14)\n",
    "plt.ylabel(\"Loss function\", fontsize=14)\n",
    "plt.legend()\n",
    "# Distance to the minimum on a logarithmic scale\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.semilogy(complexityofGD, err_gd, label=\"gd\", lw=2)\n",
    "plt.semilogy(err_sgdsr , label=\"sgd\", lw=2)\n",
    "plt.title(\"Convergence plot\", fontsize=16)\n",
    "plt.xlabel(\"# SGD iterations\", fontsize=14)\n",
    "plt.ylabel(\"Distance to the minimum\", fontsize=14)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD without replacement\n",
    "\n",
    "-Execute SGD where the indices of the data points are sampled *without* replacement over each datapass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############TODO#######################\n",
    "# Execute SGD with averaging on and shrinking stepsize. Please name the output as\n",
    "# w_sgdsw, obj_sgdsw, err_sgdsw = sgd(....)\n",
    "# HINT: You should use numpy.matlib's repmat function to  \n",
    "#import numpy.matlib\n",
    "# With replacement\n",
    "#indices = np.matlib.repmat(... )\n",
    "###############END TODO#####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error of objective on a logarithmic scale\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.yscale(\"log\")\n",
    "plt.plot(obj_sgdsr - obj_min, label=\"SGD replace\", lw=2)\n",
    "plt.plot(obj_sgdsw - obj_min, label=\"SGD withoutreplace\", lw=2)\n",
    "plt.title(\"Convergence plot\", fontsize=16)\n",
    "plt.xlabel(\"#iterations\", fontsize=14)\n",
    "plt.ylabel(\"Distance to the minimum\", fontsize=14)\n",
    "plt.legend()\n",
    "# Distance to the minimizer on a logarithmic scale\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.yscale(\"log\")\n",
    "plt.plot(err_sgdsr , label=\"SGD replace\", lw=2)\n",
    "plt.plot(err_sgdsw , label=\"SGD withoutreplace\", lw=2)\n",
    "plt.title(\"Convergence plot\", fontsize=16)\n",
    "plt.xlabel(\"#iterations\", fontsize=14)\n",
    "plt.ylabel(\"Distance to the minimum\", fontsize=14)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAG\n",
    "\n",
    "We recall that an iteration of SAG writes\n",
    "\n",
    "For $t=1, \\ldots, $ until convergence\n",
    "\n",
    "1. Pick $i_t$ uniformly at random in $\\{1, \\ldots, n\\}$\n",
    "\n",
    "2. Update the average of gradients\n",
    "$$\n",
    "g_t \\gets \\frac 1n \\sum_{i=1}^n J_i^t\n",
    "$$\n",
    "where \n",
    "$$\n",
    "J_i^t =\n",
    "\\begin{cases}\n",
    "    \\nabla f_{i}(x_t) &\\text{ if } i = i_t \\\\\n",
    "    J_i^{t-1} & \\text{ otherwise.}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "3. Apply the step \n",
    "$$x_{t+1} \\gets x_t - \\eta\\, J_t$$\n",
    "where $\\eta$ is the step-size (see code below).\n",
    "\n",
    "*QUESTION*:\n",
    "- Fill in the iteration of the SAG solver in the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sag(w0, model, indices,  step=1., w_min =0, n_iter=100, verbose = True): \n",
    "  #  (w0, model, indices, steps, w_min, n_iter=100, averaging_on=False ,verbose=True, start_late_averaging = 0):\n",
    "    \"\"\"Stochastic average gradient algorithm.\"\"\"\n",
    "    w = w0.copy()\n",
    "    # Old gradients\n",
    "    n_samples, n_features = X.shape\n",
    "    gradient_memory = np.zeros((n_samples, n_features))\n",
    "    averaged_gradient = np.zeros(n_features)\n",
    "    # estimation error history\n",
    "    errors = []\n",
    "    err = 1.\n",
    "    # objective history\n",
    "    objectives = []\n",
    "    if np.any(w_min):\n",
    "        err = norm(w - w_min) / norm(w_min)\n",
    "        errors.append(err)\n",
    "    print(\"Lauching SAG solver...\")\n",
    "    print(' | '.join([name.center(8) for name in [\"it\", \"obj\", \"err\"]]))   \n",
    "    for idx in range(n_iter):\n",
    "        i = indices[idx]    \n",
    "        ### TODO ######\n",
    "        ## Implement a step of the SAG method\n",
    "        # w[:] -= ....\n",
    "        ### END OF TODO\n",
    "        obj = model.f(w) \n",
    "        if (sum(w_min)):\n",
    "            err = norm(w - w_min) / norm(w_min)\n",
    "            errors.append(err)\n",
    "        objectives.append(obj)\n",
    "        if idx % n_samples == 0 and verbose:\n",
    "            print(' | '.join([(\"%d\" % idx).rjust(8), \n",
    "                              (\"%.2e\" % obj).rjust(8), \n",
    "                              (\"%.2e\" % err).rjust(8)]))\n",
    "    return w, np.array(objectives), np.array(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############TODO######################\n",
    "# Execute the gradient algorithm. Please name the output as\n",
    "# w_sag, obj_sag, err_sag = sag( ... )\n",
    "# Hint: Remember the stepsize of SAGA should be equal to 1/L_max, where L_max = max L_i  and L_i is the smoothness constant of f_i\n",
    "###############END TODO#####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error of objective on a logarithmic scale\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.semilogy(obj_sgdsw - obj_min, label=\"SGD withoutreplace\", lw=2)\n",
    "plt.semilogy(complexityofGD, obj_gd - obj_min, label=\"GD\", lw=2)\n",
    "plt.semilogy(obj_sag - obj_min, label=\"SAG\", lw=2)\n",
    "plt.title(\"Convergence plot\", fontsize=16)\n",
    "plt.xlabel(\"#iterations\", fontsize=14)\n",
    "plt.ylabel(\"Error of objective\", fontsize=14)\n",
    "plt.legend()\n",
    "# Distance to the minimizer on a logarithmic scale\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.yscale(\"log\")\n",
    "plt.semilogy(err_sgdsw , label=\"SGD withoutreplace\", lw=2)\n",
    "plt.semilogy(complexityofGD, err_gd , label=\"GD\", lw=2)\n",
    "plt.semilogy(err_sag , label=\"SAG\", lw=2)\n",
    "plt.title(\"Convergence plot\", fontsize=16)\n",
    "plt.xlabel(\"#iterations\", fontsize=14)\n",
    "plt.ylabel(\"Distance to the minimum\", fontsize=14)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### TODO ###### \n",
    "## Compare SGD, GD and SAG in terms of test error after performing 15 passes over the data. That is, \n",
    "# datapasses = 15\n",
    "# Then plot the progression that\n",
    "##  each method makes towards w_model_truth instead of w_min.\n",
    "## Please name the output of the methods to\n",
    "# w_sgdsw, obj_sgdsw, err_sgdsw = sgd(...  w_model_truth...)\n",
    "# w_gd, obj_gd, err_gd          = gd(... w_model_truth...)\n",
    "# w_sag, obj_sag, err_sag       = sag( ...  w_model_truth... )\n",
    "##### TODO END\n",
    "# What can you conclude? How is this plot different than the convergence to w_min? Was this to be expected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distance to the minimizer on a logarithmic scale\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.yscale(\"log\")\n",
    "plt.semilogy(err_sgdsw , label=\"SGD withoutreplace\", lw=2)\n",
    "plt.semilogy(complexityofGD, err_gd , label=\"GD\", lw=2)\n",
    "plt.semilogy(err_sag , label=\"SAG\", lw=2)\n",
    "plt.title(\"Convergence plot\", fontsize=16)\n",
    "plt.xlabel(\"#iterations\", fontsize=14)\n",
    "plt.ylabel(\"Distance to true model\", fontsize=14)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## THE END!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
